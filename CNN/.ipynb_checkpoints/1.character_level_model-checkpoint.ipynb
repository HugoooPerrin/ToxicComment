{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torchwordemb\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/Toxic Comments/classes/')\n",
    "from models import *\n",
    "from data_preprocessing import text_to_caractlist, Carac2Vect\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/hugoperrin/Bureau/Data science/Kaggle/Toxic Comments/data/preprocessed_data.pkl'\n",
    "with open(path, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)\n",
    "    train_comments = data['train_comments']\n",
    "    test_comments = data['test_comments']\n",
    "    train_labels = data['train_labels']\n",
    "    del data\n",
    "    \n",
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_comments = train_comments.reshape(train_comments.shape[0],1,train_comments.shape[1])\n",
    "test_comments = test_comments.reshape(test_comments.shape[0],1,test_comments.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_comments = train_comments[80000:,:,:]\n",
    "train_comments = train_comments[:80000,:,:]\n",
    "\n",
    "valid_labels = train_labels[80000:,:]\n",
    "train_labels = train_labels[:80000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_comments = torch.FloatTensor(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> PROCESSING LEARNING: 1511781 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.0765, validation loss: 0.04846\n",
      "Epoch: 2, step:   200, training loss: 0.0179, validation loss: 0.04804\n",
      "Epoch: 2, step:   300, training loss: 0.0507, validation loss: 0.04826\n",
      "Epoch: 3, step:   400, training loss: 0.0401, validation loss: 0.04759\n",
      "Epoch: 4, step:   500, training loss: 0.0111, validation loss: 0.04874\n",
      "Epoch: 4, step:   600, training loss: 0.0486, validation loss: 0.05050\n",
      "Epoch: 5, step:   700, training loss: 0.0339, validation loss: 0.05129\n",
      "\n",
      "\n",
      "CPU times: user 22.9 s, sys: 8.04 s, total: 31 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "target = 'identity_hate'\n",
    "\n",
    "labels_train = train_labels[:,list_classes.index(target)]\n",
    "labels_train = labels_train.reshape(labels_train.shape[0],1)\n",
    "\n",
    "labels_valid = valid_labels[:,list_classes.index(target)]\n",
    "labels_valid = labels_valid.reshape(labels_valid.shape[0],1)\n",
    "\n",
    "num_epoch = 5\n",
    "use_GPU = True\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(train_comments), \n",
    "                                               torch.FloatTensor(labels_train))\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(valid_comments), \n",
    "                                               torch.FloatTensor(labels_valid))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers = 8)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False, \n",
    "                                           num_workers = 8)\n",
    "\n",
    "net = CNN().cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.00001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0.9)\n",
    "\n",
    "train(num_epoch, net, train_loader, optimizer, criterion, valid_loader=valid_loader)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = predict(net, test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame()\n",
    "# submission['id'] = final_id\n",
    "\n",
    "# submission.to_csv('/home/hugoperrin/Bureau/Data science/Numerai/w90/submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
